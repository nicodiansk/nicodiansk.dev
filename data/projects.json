{
  "projects": [
    {
      "id": "dev-velocity-mcp",
      "status": "LIVE",
      "evolution": {
        "en": "2024 → Present (agents doing the heavy lifting)",
        "it": "2024 → Presente (gli agenti fanno il lavoro pesante)"
      },
      "title": {
        "en": "DEV_VELOCITY_MCP",
        "it": "DEV_VELOCITY_MCP"
      },
      "description": {
        "en": "SSOT via Confluence+Slack+Jira → AI coding assistants",
        "it": "SSOT via Confluence+Slack+Jira → assistenti AI per coding"
      },
      "longDescription": {
        "en": "Architected MCP-based system integrating Confluence, Slack, and Jira as Single Source of Truth for AI coding assistants. Designed custom agents with single-scope responsibilities (code review, docs ingestion, context building) using advanced prompt engineering, orchestrating Claude + Gemini cooperation for context-aware development.",
        "it": "Sistema MCP che integra Confluence, Slack e Jira come Single Source of Truth per assistenti AI di coding. Agenti personalizzati con responsabilità specifiche (code review, docs ingestion, context building) usando prompt engineering avanzato, orchestrando cooperazione Claude + Gemini per sviluppo context-aware."
      },
      "techStack": ["Claude", "Gemini", "LangGraph", "MCP", "LangChain"],
      "metrics": [
        {
          "label": { "en": "Dev Productivity", "it": "Produttività Dev" },
          "value": "↑ 60-70%",
          "color": "lime"
        }
      ],
      "tags": ["multi-agent", "mcp", "orchestration"],
      "year": 2024,
      "featured": true
    },
    {
      "id": "geo-seo-engine",
      "status": "LIVE",
      "evolution": {
        "en": "2024 (because humans hate writing JSON-LD)",
        "it": "2024 (perché gli umani odiano scrivere JSON-LD)"
      },
      "title": {
        "en": "GEO_SEO_ENGINE",
        "it": "GEO_SEO_ENGINE"
      },
      "description": {
        "en": "Auto-gen product content • FAQs • JSON-LD • multilingual",
        "it": "Auto-gen contenuti prodotti • FAQ • JSON-LD • multilingua"
      },
      "longDescription": {
        "en": "Built end-to-end AI pipeline for automatic product enhancement: descriptions, FAQs, structured data (JSON-LD), and multilingual tone-of-voice adaptation. AI-powered product feed optimization using latest OpenAI models for GEO (Generative Engine Optimization) and SEO discoverability.",
        "it": "Pipeline AI end-to-end per enhancement automatico prodotti: descrizioni, FAQ, dati strutturati (JSON-LD), e adattamento multilingua del tono di voce. Ottimizzazione feed prodotti con modelli OpenAI per GEO (Generative Engine Optimization) e SEO."
      },
      "techStack": ["OpenAI", "FastAPI", "Knowledge Graphs", "NLP"],
      "metrics": [
        {
          "label": { "en": "Content Opt Time", "it": "Tempo Opt Contenuti" },
          "value": "↓ 80%",
          "color": "cyan"
        }
      ],
      "tags": ["seo", "geo", "e-commerce"],
      "year": 2024,
      "featured": true
    },
    {
      "id": "legal-rag-v2",
      "status": "LIVE",
      "evolution": {
        "en": "2023 → 2025 (plot twist: now with graphs)",
        "it": "2023 → 2025 (colpo di scena: ora con grafi)"
      },
      "title": {
        "en": "LEGAL_RAG_v2",
        "it": "LEGAL_RAG_v2"
      },
      "description": {
        "en": "100K+ docs • Neo4j knowledge graph • cross-citation",
        "it": "100K+ doc • knowledge graph Neo4j • cross-citation"
      },
      "longDescription": {
        "en": "Multi-year RAG platform evolution (2023→2025) for ~100K Italian legal documents. Started 2023 with foundational retrieval architecture and hybrid search (semantic + lexical). Major 2024-2025 upgrade: integrated Neo4j knowledge graphs for legal entity relationships, cross-citation networks, and precedent tracking. Current stack includes multimodal RAG, domain-specific embeddings, vector databases, and optimized context window management for large-scale legal document processing.",
        "it": "Evoluzione piattaforma RAG multi-anno (2023→2025) per ~100K documenti legali italiani. Partenza 2023 con architettura retrieval e hybrid search (semantica + lessicale). Upgrade maggiore 2024-2025: integrazione Neo4j knowledge graphs per relazioni entità legali, reti cross-citation e tracking precedenti. Stack attuale include RAG multimodale, embeddings domain-specific, vector databases e gestione ottimizzata context window per processing documenti legali su larga scala."
      },
      "techStack": ["LangChain", "Neo4j", "Pinecone", "GPT-4", "Hybrid Search"],
      "metrics": [
        {
          "label": { "en": "Recall@10", "it": "Recall@10" },
          "value": "91%",
          "color": "yellow"
        },
        {
          "label": { "en": "Answer Relevance", "it": "Rilevanza Risposte" },
          "value": "89%",
          "color": "magenta"
        }
      ],
      "tags": ["rag", "legal", "neo4j", "knowledge-graph"],
      "year": 2023,
      "featured": true
    },
    {
      "id": "vision-classify",
      "status": "LIVE",
      "evolution": {
        "en": "2023 (teaching computers to shop)",
        "it": "2023 (insegnando ai computer a fare shopping)"
      },
      "title": {
        "en": "VISION_CLASSIFY",
        "it": "VISION_CLASSIFY"
      },
      "description": {
        "en": "CNN page classification • image similarity • product discovery",
        "it": "Classificazione pagine CNN • similarity immagini • product discovery"
      },
      "longDescription": {
        "en": "Developed full ML pipeline—data collection, labeling, training, validation, and monitoring—for classifying e-commerce page types using CNN-based computer vision models. Transfer learning and custom architectures for image classification, object detection, and similarity search. Integrated with Qdrant vector database for fast similarity retrieval.",
        "it": "Pipeline ML completa—raccolta dati, labeling, training, validazione, monitoring—per classificare tipologie pagine e-commerce usando modelli CNN di computer vision. Transfer learning e architetture custom per classificazione immagini, object detection e similarity search. Integrato con Qdrant vector database per retrieval veloce."
      },
      "techStack": ["PyTorch", "Qdrant", "Transfer Learning", "ONNX"],
      "metrics": [
        {
          "label": { "en": "Manual Work", "it": "Lavoro Manuale" },
          "value": "↓ 100%",
          "color": "magenta"
        }
      ],
      "tags": ["computer-vision", "cnn", "e-commerce"],
      "year": 2023,
      "featured": false
    },
    {
      "id": "hr-assistant",
      "status": "LIVE",
      "evolution": {
        "en": "2023 → 2025 (RAG got an upgrade)",
        "it": "2023 → 2025 (RAG ha fatto l'upgrade)"
      },
      "title": {
        "en": "HR_ASSISTANT",
        "it": "HR_ASSISTANT"
      },
      "description": {
        "en": "RAG-powered HR assistant • vector search • Supabase",
        "it": "Assistente HR con RAG • vector search • Supabase"
      },
      "longDescription": {
        "en": "Multi-year HR chatbot evolution (2023→2025) enabling employees to access HR and company information in natural language. Started 2023 with rule-based NLP query system for basic HR lookups. Major 2025 upgrade: migrated to full RAG architecture with Supabase vector store and semantic search capabilities. Current features: PTO balance queries, room booking, newsletter summaries, policy lookups with vector search. Integrated with internal systems via API, guardrails, and human-in-the-loop escalation.",
        "it": "Evoluzione chatbot HR multi-anno (2023→2025) per accesso info HR e aziendali in linguaggio naturale. Partenza 2023 con sistema query NLP rule-based per lookup HR base. Upgrade maggiore 2025: migrazione ad architettura RAG completa con Supabase vector store e semantic search. Features attuali: query ferie, prenotazioni, riassunti newsletter, lookup policy con vector search. Integrato con sistemi interni via API, guardrails ed escalation human-in-the-loop."
      },
      "techStack": ["LangChain", "Supabase", "RAG", "Vector Search", "FastAPI"],
      "metrics": [
        {
          "label": { "en": "Query Accuracy", "it": "Accuratezza Query" },
          "value": "↑ 45%",
          "color": "cyan"
        }
      ],
      "tags": ["rag", "chatbot", "hr-automation", "vector-search"],
      "year": 2023,
      "featured": false
    },
    {
      "id": "hybrid-recsys",
      "status": "LIVE",
      "evolution": {
        "en": "2022 (when speed was everything)",
        "it": "2022 (quando la velocità era tutto)"
      },
      "title": {
        "en": "HYBRID_RECSYS",
        "it": "HYBRID_RECSYS"
      },
      "description": {
        "en": "Sub-100ms recs • collaborative + vector search",
        "it": "Rec sub-100ms • collaborative + vector search"
      },
      "longDescription": {
        "en": "Built low-latency recommendation engine combining collaborative filtering with vector similarity search. Hybrid system using semantic ranking, embeddings, vector search, and knowledge-graph signals for personalized product discovery. Optimized for minimal compute overhead while maintaining sub-100ms response times.",
        "it": "Motore raccomandazioni low-latency combinando collaborative filtering con vector similarity search. Sistema ibrido con semantic ranking, embeddings, vector search e segnali knowledge-graph per product discovery personalizzata. Ottimizzato per minimo overhead computazionale mantenendo tempi risposta sub-100ms."
      },
      "techStack": ["Embeddings", "Custom Rankers", "Vector Search", "Collaborative Filtering"],
      "metrics": [
        {
          "label": { "en": "Response Time", "it": "Tempo Risposta" },
          "value": "<100ms",
          "color": "lime"
        }
      ],
      "tags": ["recommendation", "hybrid-search", "low-latency"],
      "year": 2022,
      "featured": false
    }
  ]
}
